{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1eedb8e-69fc-4495-a313-fe81aaf59c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a4332-dfe5-45a6-bb57-86949b6856de",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D, Input, Dropout\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'batch_size': [32, 64],\n",
    "    'learning_rate': [1e-4, 1e-3, 1e-2, 1e-1],\n",
    "    'dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'l2_regularizer': [1e-5, 1e-4, 1e-3],\n",
    "    'dense_units': [128, 256, 512],\n",
    "    'trainable': [False, True],\n",
    "    'image_size': [224]\n",
    "}\n",
    "\n",
    "# Generate all combinations of parameters\n",
    "combinations = list(product(*param_grid.values()))\n",
    "param_names = list(param_grid.keys())\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Ensure y_true is float32\n",
    "    y_pred = tf.cast(y_pred, tf.float32)  # Ensure y_pred is float32\n",
    "    \n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred))  # Residual sum of squares\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))  # Total sum of squares\n",
    "    r2 = 1 - ss_res / (ss_tot + tf.keras.backend.epsilon())  # Add epsilon for stability\n",
    "    return r2\n",
    "\n",
    "\n",
    "for combination in combinations:\n",
    "    params = dict(zip(param_names, combination))\n",
    "    \n",
    "    # Unpack parameters\n",
    "    batch_size = params['batch_size']\n",
    "    learning_rate = params['learning_rate']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    l2_regularizer = params['l2_regularizer']\n",
    "    dense_units = params['dense_units']\n",
    "    trainable = params['trainable']\n",
    "    height = params['image_size']\n",
    "    width = params['image_size']\n",
    "    epochs = 10\n",
    "    \n",
    "    print(f\"Training with parameters: {params}\")\n",
    "\n",
    "    # Path to dataset (adjust accordingly)\n",
    "    # train_dir = f'{os.getcwd()}/train'\n",
    "    train_dir = 'data/train'\n",
    "    val_dir = 'data/val'\n",
    "    \n",
    "    # Path to CSV with filenames and labels\n",
    "    # Load the label data\n",
    "    train_labels = pd.read_csv('data/train.csv')\n",
    "    val_labels = pd.read_csv('data/val.csv')\n",
    "    \n",
    "    # Function to preprocess images\n",
    "    def preprocess_image(image_path, label):\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, [height, width])  # ResNet-50 input size\n",
    "        image = tf.keras.applications.resnet50.preprocess_input(image)  # Normalize\n",
    "        return image, label\n",
    "    \n",
    "    # Function to map file paths\n",
    "    def get_image_path_and_label(base_dir, df):\n",
    "        file_paths = df['image_name'].apply(lambda x: os.path.join(base_dir, x)).values\n",
    "        labels = df['label'].values\n",
    "        return file_paths, labels\n",
    "    \n",
    "    # Get file paths and labels\n",
    "    train_image_paths, train_labels = get_image_path_and_label(train_dir, train_labels)\n",
    "    val_image_paths, val_labels = get_image_path_and_label(val_dir, val_labels)\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((train_image_paths, train_labels))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((val_image_paths, val_labels))\n",
    "    \n",
    "    # Preprocess the datasets\n",
    "    train_ds = train_ds.map(preprocess_image).batch(batch_size).shuffle(buffer_size=1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val_ds = val_ds.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Load pre-trained ResNet 50 model\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(height, width, 3)\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = trainable\n",
    "    \n",
    "    # Add custom layers for classification\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_regularizer)),\n",
    "        Dropout(0.5),  # Helps prevent overfitting\n",
    "        Dense(dense_units // 2, activation='relu', kernel_regularizer=l2(l2_regularizer)),\n",
    "        Dense(1, activation='linear')  # Output layer for regression\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='mse', \n",
    "        metrics=['mape', 'mae', r2_score]\n",
    "    )\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "# Sort results by validation loss\n",
    "sorted_results = sorted(results, key=lambda x: x['val_loss'])\n",
    "print(\"Best Parameters:\", sorted_results[0]['params'])\n",
    "print(\"Best Validation Loss:\", sorted_results[0]['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6659d6-30b2-42a1-9386-7c133c0f2be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
